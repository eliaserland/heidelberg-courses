{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12\n",
    "## Time Series Analysis & Recurrent Neural Networks, SoSe 2021\n",
    "### Author: Elias Olofsson\n",
    "    Version information:\n",
    "        2021-07-14: v.1.0. First public release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the Evidence Lower bound, we have:\n",
    "\\begin{align}\n",
    "        \\operatorname{ELBO} &= \\mathbb{E}_{z \\sim q(z|x)}\\left[\\log p(x|z)\\right]-D_{\\mathrm{KL}}\\left[q(z|x) \\| p(z)\\right]\\\\\n",
    "        &= \\int q(z|x) \\log p(x|z) dz - \\int q(z|x) \\log \\left(\\frac{q(z|x)}{p(z)}\\right) dz\\\\\n",
    "        &= \\int q(z|x) \\left(\\log p(x|z) + \\log \\frac{p(z)}{q(z|x)}  \\right) dz\\\\\n",
    "        &= \\int q(z|x) \\log \\left(\\frac{p(x|z)p(z)}{q(z|x)}\\right) dz\\\\\n",
    "        &= \\int q(z|x) \\log \\left(\\frac{p(z|x)p(x)}{q(z|x)}\\right) dz\\\\\n",
    "        &= \\int q(z|x) \\log p(x) dz - \\int q(z|x) \\log \\frac{q(z|x)}{p(z|x)} dz\\\\\n",
    "        &= \\log p(x) \\underbrace{\\int q(z|x) dz}_\\text{=1} - D_{\\mathrm{KL}} \\left[q(z|x)\\|p(z|x)\\right]\\\\\n",
    "        &= \\underbrace{\\log p(x)}_{\\substack{\\text{const. w.r.t.} \\\\ \\text{optim. obj.}}} - D_{\\mathrm{KL}} \\left[q(z|x)\\|p(z|x)\\right]\n",
    "\\end{align}\n",
    "where we used Bayes's rule, the definition of the Kullbackâ€“Leibler divergence and the fact that the proposal density $q(z|x)$ is normalized. The conclusion is that maximazation of the ELBO is equivialent to minimizing\n",
    "\\begin{equation}\n",
    "    D_{\\mathrm{KL}} \\left[q(z|x)\\|p(z|x)\\right], \n",
    "\\end{equation}\n",
    "since $p(x)$ is independent of the optimization objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rc('image', cmap='gray')\n",
    "import pickle\n",
    "\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Notebook graphics settings: \n",
    "%config InlineBackend.figure_format = 'svg' # inline graphics (options: 'svg', 'png', 'retina', etc.)\n",
    "plt.rcParams['figure.dpi'] = 200            # custom dpi setting for inline png:s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_x, dim_h, dim_z):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim_x, dim_h)\n",
    "        self.mu = nn.Linear(dim_h, dim_z)\n",
    "        self.logvar = nn.Linear(dim_h, dim_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear(x))\n",
    "        z_mu = self.mu(h)\n",
    "        z_logvar = self.logvar(h)\n",
    "        return z_mu, z_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_z, dim_h, dim_x):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim_z, dim_h)\n",
    "        self.linear2 = nn.Linear(dim_h, dim_x)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.linear1(z))\n",
    "        x = tc.sigmoid(self.linear2(h))\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mu, z_logvar = self.enc(x)\n",
    "        z_sample = reparametrize(z_mu, z_logvar)\n",
    "        x_sample = self.dec(z_sample)\n",
    "        return x_sample, z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(z_mu, z_logvar):\n",
    "    z_sample = NotImplemented\n",
    "    return z_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_evidence_lower_bound(x, x_sample, z_mu, z_logvar):\n",
    "    rec_loss = NotImplemented\n",
    "    kl_loss = NotImplemented\n",
    "    loss = rec_loss + kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        x_sample, z_mu, z_logvar = model(x)\n",
    "        loss = negative_evidence_lower_bound(x, x_sample, z_mu, z_logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return train_loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with tc.no_grad():  # no need to track the gradients here\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            z_sample, z_mu, z_var = model(x)\n",
    "            loss = negative_evidence_lower_bound(x, z_sample, z_mu, z_var)\n",
    "            test_loss += loss.item()\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # number of data points in each batch\n",
    "n_epochs   = 10  # times to run the model on complete data\n",
    "dim_x = 28*28    # size of each input\n",
    "dim_h = 256      # hidden dimension\n",
    "dim_z = 50       # latent vector dimension\n",
    "lr    = 1e-3     # learning rate\n",
    "\n",
    "device = tc.device('cuda' if tc.cuda.is_available() else 'cpu')\n",
    "transforms = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = datasets.MNIST('./data', train=True, download=True, transform=transforms)\n",
    "test_set = datasets.MNIST('./data', train=False, download=True, transform=transforms)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(dim_x, dim_h, dim_z)\n",
    "decoder = Decoder(dim_z, dim_h, dim_x)\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "    train_loss /= len(train_set)\n",
    "    test_loss /= len(test_set)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd0191b2412b5c825941a96bf30deef8bd9426d7fee1eaf0f751783c44781e2bb9d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
